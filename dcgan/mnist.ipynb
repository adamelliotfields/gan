{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN\n",
    "\n",
    "[![arXiv](https://img.shields.io/badge/arXiv-1511.06434-b31b1b?logo=arxiv)](https://arxiv.org/abs/1511.06434)\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adamelliotfields/gan/blob/main/dcgan/mnist.ipynb)\n",
    "[![Render nbviewer](https://img.shields.io/badge/render-nbviewer-f37726)](https://nbviewer.org/github/adamelliotfields/gan/blob/main/dcgan/mnist.ipynb)\n",
    "[![W&B](https://img.shields.io/badge/Weights_&_Biases-FFCC33?logo=WeightsAndBiases&logoColor=black)](https://wandb.ai/adamelliotfields/gan-mnist)\n",
    "\n",
    "Based on the official Keras [tutorial](https://keras.io/examples/generative/dcgan_overriding_train_step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Config\n",
    "SEED = 42  # @param {type:\"integer\"}\n",
    "VERBOSE = 1  # @param {type:\"integer\"}\n",
    "EPOCHS = 100  # @param {type:\"integer\"}\n",
    "LATENT_DIM = 100  # @param {type:\"integer\"}\n",
    "BATCH_SIZE = 256  # @param {type:\"integer\"}\n",
    "JIT_COMPILE = True  # @param {type:\"boolean\"}\n",
    "DTYPE_POLICY = \"mixed_float16\"  # @param [\"float32\", \"mixed_float16\"] {type:\"string\"}\n",
    "\n",
    "# lower learning rate and beta_1 for Adam optimizer\n",
    "# beta_1 (m_t in the paper) is the first moment estimate\n",
    "LEARNING_RATE = 0.0002  # @param {type:\"number\"}\n",
    "BETA_1 = 0.5  # @param {type:\"number\"}\n",
    "\n",
    "# https://wandb.ai/{ENTITY}/{PROJECT}\n",
    "WANDB_ENTITY = \"adamelliotfields\"  # @param {type:\"string\"}\n",
    "WANDB_PROJECT = \"gan-mnist\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Environment\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "\n",
    "    try:\n",
    "        # only log to W&B if there's a key\n",
    "        os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB_API_KEY\")\n",
    "        os.environ[\"WANDB_DISABLE_GIT\"] = \"true\"\n",
    "    except (userdata.NotebookAccessError, userdata.SecretNotFoundError):\n",
    "        pass\n",
    "\n",
    "    GOOGLE_DRIVE_DIR = \"/content/drive/MyDrive\"\n",
    "    MODEL_SAVE_DIR = os.path.join(GOOGLE_DRIVE_DIR, \"keras\", \"models\")\n",
    "    TENSORBOARD_LOG_DIR = os.path.join(GOOGLE_DRIVE_DIR, \"tensorboard\")\n",
    "    os.environ[\"TFDS_DATA_DIR\"] = os.path.join(GOOGLE_DRIVE_DIR, \"tensorflow_datasets\")\n",
    "    subprocess.run([\"pip\", \"install\", \"-qU\", \"keras\", \"wandb\"])\n",
    "except ImportError:\n",
    "    MODEL_SAVE_DIR = \"./\"\n",
    "    TENSORBOARD_LOG_DIR = \"./logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "import math\n",
    "import wandb\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from datetime import datetime\n",
    "from wandb.integration.keras import WandbMetricsLogger\n",
    "\n",
    "from keras import (\n",
    "    Input,\n",
    "    Model,\n",
    "    Sequential,\n",
    "    backend,\n",
    "    callbacks,\n",
    "    config,\n",
    "    layers,\n",
    "    losses,\n",
    "    metrics,\n",
    "    ops,\n",
    "    optimizers,\n",
    "    random,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Functions\n",
    "def generate_predictions(generator, n=4):\n",
    "    seed = random.normal((n, LATENT_DIM))\n",
    "    predictions = generator(seed, training=False)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def plot_predictions(predictions, figsize=(2, 2)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(figsize[0], figsize[1], i + 1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Data\n",
    "(mnist_train, mnist_test), mnist_info = tfds.load(\n",
    "    \"mnist\",\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    "    split=[\"train\", \"test\"],\n",
    ")\n",
    "X_train = (\n",
    "    mnist_train.concatenate(mnist_test)\n",
    "    .map(\n",
    "        # normalize to -1,1 and remove label\n",
    "        lambda x, _: (ops.cast(x, \"float32\") - 127.5) / 127.5\n",
    "    )\n",
    "    .shuffle(70000)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title DCGAN\n",
    "class DCGAN(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.seed_generator = random.SeedGenerator(SEED)\n",
    "        self.G_loss_metric = metrics.Mean(name=\"g_loss\")\n",
    "        self.D_loss_metric = metrics.Mean(name=\"d_loss\")\n",
    "        self.loss = losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "        # we need to train the generator and discriminator together\n",
    "        # we'll handle computing and applying gradients manually, so we won't need to compile them\n",
    "        self.generator = Sequential(\n",
    "            [\n",
    "                # use BN-ReLU in generator; dropout in discriminator\n",
    "                # normally, bias terms are added to the outputs of the layer to offset\n",
    "                # batch normalization centers the data, which cancels out any offset\n",
    "                # therefore, we don't need bias terms in layers with BN\n",
    "                Input(shape=(LATENT_DIM,)),\n",
    "                layers.Dense(7 * 7 * 256, use_bias=False),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.LeakyReLU(),\n",
    "                layers.Reshape((7, 7, 256)),\n",
    "                # use transpose (deconvolution) instead of upsample + convolution\n",
    "                # upsampling uses a fixed technique like bilinear interpolation that isn't learnable\n",
    "                # so transposed convolution is like a learnable upsampling\n",
    "                layers.Conv2DTranspose(\n",
    "                    128,\n",
    "                    strides=1,\n",
    "                    kernel_size=5,\n",
    "                    padding=\"same\",\n",
    "                    use_bias=False,\n",
    "                ),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.LeakyReLU(),\n",
    "                layers.Conv2DTranspose(\n",
    "                    64,\n",
    "                    strides=2,\n",
    "                    kernel_size=5,\n",
    "                    padding=\"same\",\n",
    "                    use_bias=False,\n",
    "                ),\n",
    "                layers.BatchNormalization(),\n",
    "                layers.LeakyReLU(),\n",
    "                # tanh activation because we normalized the discriminator's training images to -1,1\n",
    "                # if we normalized to 0,1 we would use sigmoid here\n",
    "                # (-1,1 is recommended)\n",
    "                layers.Conv2DTranspose(\n",
    "                    1,\n",
    "                    strides=2,\n",
    "                    kernel_size=5,\n",
    "                    padding=\"same\",\n",
    "                    activation=\"tanh\",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        self.discriminator = Sequential(\n",
    "            [\n",
    "                Input(shape=(28, 28, 1)),\n",
    "                layers.Conv2D(64, strides=2, kernel_size=5, padding=\"same\"),\n",
    "                layers.LeakyReLU(),\n",
    "                layers.Dropout(0.3),\n",
    "                layers.Conv2D(128, strides=2, kernel_size=5, padding=\"same\"),\n",
    "                layers.LeakyReLU(),\n",
    "                layers.Dropout(0.3),\n",
    "                layers.Flatten(),\n",
    "                layers.Dense(1),  # no activation so use from_logits\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # anything that can change during recompilation should be in compile\n",
    "    def compile(self, jit_compile=\"auto\", learning_rate=1e-3, beta_1=0.9):\n",
    "        super().compile(jit_compile=jit_compile)\n",
    "        self.G_optimizer = optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "        self.D_optimizer = optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.G_loss_metric, self.D_loss_metric]\n",
    "\n",
    "    def G_loss(self, fake_output):\n",
    "        return self.loss(ops.ones_like(fake_output), fake_output)\n",
    "\n",
    "    def D_loss(self, real_output, fake_output):\n",
    "        real_loss = self.loss(ops.ones_like(real_output), real_output)\n",
    "        fake_loss = self.loss(ops.zeros_like(fake_output), fake_output)\n",
    "        return real_loss + fake_loss\n",
    "\n",
    "    def train_step(self, images):\n",
    "        # 256 vectors of 100 random floats each\n",
    "        noise = random.normal((BATCH_SIZE, LATENT_DIM), seed=self.seed_generator)\n",
    "\n",
    "        # use a single context to avoid duplicate code\n",
    "        with tf.GradientTape() as G_tape, tf.GradientTape() as D_tape:\n",
    "            generated_images = self.generator(noise, training=True)\n",
    "            real_output = self.discriminator(images, training=True)\n",
    "            fake_output = self.discriminator(generated_images, training=True)\n",
    "            G_loss_metric = self.G_loss(fake_output)\n",
    "            D_loss_metric = self.D_loss(real_output, fake_output)\n",
    "\n",
    "        G_grads = G_tape.gradient(G_loss_metric, self.generator.trainable_variables)\n",
    "        D_grads = D_tape.gradient(D_loss_metric, self.discriminator.trainable_variables)\n",
    "\n",
    "        self.G_optimizer.apply_gradients(zip(G_grads, self.generator.trainable_variables))\n",
    "        self.D_optimizer.apply_gradients(zip(D_grads, self.discriminator.trainable_variables))\n",
    "\n",
    "        self.G_loss_metric.update_state(G_loss_metric)\n",
    "        self.D_loss_metric.update_state(D_loss_metric)\n",
    "        return {\"g_loss\": self.G_loss_metric.result(), \"d_loss\": self.D_loss_metric.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Compile\n",
    "backend.clear_session()\n",
    "config.set_dtype_policy(DTYPE_POLICY)\n",
    "dcgan = DCGAN()\n",
    "dcgan.compile(\n",
    "    beta_1=BETA_1,\n",
    "    jit_compile=JIT_COMPILE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Train\n",
    "tensorboard_cb = callbacks.TensorBoard(\n",
    "    log_dir=os.path.join(\n",
    "        TENSORBOARD_LOG_DIR,\n",
    "        WANDB_PROJECT,\n",
    "        datetime.now().strftime(\"%Y%m%d%H%M%S\"),\n",
    "    )\n",
    ")\n",
    "checkpoint_cb = callbacks.ModelCheckpoint(\n",
    "    os.path.join(MODEL_SAVE_DIR, \"dcgan-mnist.weights.h5\"),\n",
    "    save_weights_only=True,\n",
    "    save_freq=math.ceil(70000 / BATCH_SIZE) * EPOCHS,  # only save after training\n",
    ")\n",
    "\n",
    "if not os.environ.get(\"WANDB_API_KEY\"):\n",
    "    dcgan.fit(\n",
    "        X_train,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=VERBOSE,\n",
    "        callbacks=[tensorboard_cb, checkpoint_cb],\n",
    "    )\n",
    "    predictions = generate_predictions(dcgan.generator, n=25)\n",
    "    fig = plot_predictions(predictions, figsize=(5, 5))\n",
    "else:\n",
    "    with wandb.init(\n",
    "        group=\"dcgan\",\n",
    "        job_type=\"train\",\n",
    "        entity=WANDB_ENTITY,\n",
    "        project=WANDB_PROJECT,\n",
    "        sync_tensorboard=False,\n",
    "        config={\n",
    "            \"model\": \"DCGAN\",\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"beta_1\": BETA_1,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"latent_dim\": LATENT_DIM,\n",
    "            \"dtype_policy\": DTYPE_POLICY,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "        },\n",
    "    ) as run:\n",
    "        dcgan.fit(\n",
    "            X_train,\n",
    "            epochs=EPOCHS,\n",
    "            verbose=VERBOSE,\n",
    "            callbacks=[tensorboard_cb, checkpoint_cb, WandbMetricsLogger()],\n",
    "        )\n",
    "        predictions = generate_predictions(dcgan.generator, n=25)\n",
    "        fig = plot_predictions(predictions, figsize=(5, 5))\n",
    "        fig.savefig(\"generations.png\")\n",
    "        run.log({\"Generations (DCGAN)\": wandb.Image(\"generations.png\")})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
